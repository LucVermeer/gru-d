{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some experiments to check data distributions and model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "from torch.utils.data import random_split\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from model import GRUD\n",
    "from data import TimeSeriesDataset\n",
    "\n",
    "from datetime import datetime\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "seq_len = 100\n",
    "step_size = 10\n",
    "\n",
    "def split_data(\n",
    "    df,\n",
    "    start_test=\"2023-06-14 07:36:33.297403\",\n",
    "    end_test=\"2023-06-14 08:03:30.700492\",\n",
    "):\n",
    "    \"\"\"The timestamps are in column 'Time (s)\"\"\"\n",
    "    # Definne the test set as the rows in between the start_test and end_test timestamps\n",
    "    test_df = df[(df[\"Time (s)\"] >= start_test) & (df[\"Time (s)\"] <= end_test)]\n",
    "    # Define the train set as the rows before the start_test timestamp and after the end_test timestamp\n",
    "    train_df = df[(df[\"Time (s)\"] < start_test) | (df[\"Time (s)\"] > end_test)]\n",
    "    return train_df, test_df\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Split the dataframe into train, validation, and test\n",
    "train_df, test_df = split_data(df)\n",
    "# create the training dataset\n",
    "train_dataset = TimeSeriesDataset(\n",
    "    train_df\n",
    ")\n",
    "\n",
    "# create the test dataset using the scaler from the training dataset\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_df,\n",
    "    scaler=train_dataset.get_scaler(),\n",
    "    label_encoder=train_dataset.get_label_encoder(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lverm\\Documents\\Courses\\2023_SEM_2\\ML4QS\\main\\grud\\experiments.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lverm/Documents/Courses/2023_SEM_2/ML4QS/main/grud/experiments.ipynb#X10sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mmodels/model_42_20230622-181251.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lverm/Documents/Courses/2023_SEM_2/ML4QS/main/grud/experiments.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m model \u001b[39m=\u001b[39m GRUD(input_size\u001b[39m=\u001b[39m\u001b[39m17\u001b[39m, hidden_size\u001b[39m=\u001b[39m\u001b[39m17\u001b[39m, output_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lverm/Documents/Courses/2023_SEM_2/ML4QS/main/grud/experiments.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39;49m\u001b[39mstate_dict\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lverm/Documents/Courses/2023_SEM_2/ML4QS/main/grud/experiments.ipynb#X10sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lverm/Documents/Courses/2023_SEM_2/ML4QS/main/grud/experiments.ipynb#X10sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(test_dataset)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'state_dict'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def metrics(labels, predictions):\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average='weighted')\n",
    "    recall = recall_score(labels, predictions, average='weighted')\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-Score:\", f1)\n",
    "    \n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    class_labels = test_dataset.get_label_encoder().classes_\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_context(\"paper\")\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.set(font_scale=1.5)\n",
    "    sns.heatmap(cm, \n",
    "                annot=True, \n",
    "                cmap=\"Blues\", \n",
    "                fmt=\"d\", \n",
    "                xticklabels=class_labels, \n",
    "                yticklabels=class_labels, \n",
    "                cbar=False\n",
    "    )\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.savefig(\"confusion_matrix.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Load the model from grud\\models\\model_42_20230622-181251.pt\n",
    "checkpoint = torch.load('models/model_42_20230622-181251.pt')\n",
    "model = GRUD(input_size=17, hidden_size=17, output_size=3)\n",
    "print(checkpoint['state_dict'].keys())\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(test_dataset)\n",
    "\n",
    "# Get the labels from the test set with the label encoder\n",
    "labels = test_dataset.get_labels()\n",
    "\n",
    "# Calculate the metrics\n",
    "metrics(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: torch.Size([3215, 100, 17])\n",
      "Test set shape: torch.Size([1607, 100, 17])\n",
      "Label distributions:\n",
      "Training set: [2294    7  100   84  193  537]\n",
      "Test set: [979  26 118 123 258 103]\n",
      "Training set: {0, 1, 2, 3, 4, 5}\n",
      "Test set: {0, 1, 2, 3, 4, 5}\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the datasets\n",
    "print(f\"Training set shape: {train_dataset.x.shape}\")\n",
    "# print(f\"Validation set shape: {val_dataset.x.shape}\")\n",
    "print(f\"Test set shape: {test_dataset.x.shape}\")\n",
    "\n",
    "# # Print the first few labels in each dataset\n",
    "# print(\"First few labels:\")\n",
    "# print(\"Training set:\", train_dataset.y[:50].numpy())\n",
    "# # print(\"Validation set:\", val_dataset.y[:10].numpy())\n",
    "# print(\"Test set:\", test_dataset.y[:10].numpy())\n",
    "\n",
    "\n",
    "# Print the distributions of the labels in each dataset\n",
    "print(\"Label distributions:\")\n",
    "print(\"Training set:\", np.bincount(train_dataset.y.numpy()))\n",
    "# print(\"Validation set:\", np.bincount(val_dataset.y.numpy()))\n",
    "print(\"Test set:\", np.bincount(test_dataset.y.numpy()))\n",
    "\n",
    "# Print the datasets as sets\n",
    "print(\"Training set:\", set(train_dataset.y.numpy()))\n",
    "# print(\"Validation set:\", set(val_dataset.y.numpy()))\n",
    "print(\"Test set:\", set(test_dataset.y.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lverm\\Documents\\Courses\\2023_SEM_2\\ML4QS\\main\\grud\\experiments.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lverm/Documents/Courses/2023_SEM_2/ML4QS/main/grud/experiments.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Calculate missing values\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lverm/Documents/Courses/2023_SEM_2/ML4QS/main/grud/experiments.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m missing_values_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(\u001b[39m~\u001b[39mtrain_dataset\u001b[39m.\u001b[39mmask\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(\u001b[39mbool\u001b[39m)) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39mprod(train_dataset\u001b[39m.\u001b[39mmask\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lverm/Documents/Courses/2023_SEM_2/ML4QS/main/grud/experiments.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m missing_values_val \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(\u001b[39m~\u001b[39mval_dataset\u001b[39m.\u001b[39mmask\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(\u001b[39mbool\u001b[39m)) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39mprod(val_dataset\u001b[39m.\u001b[39mmask\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lverm/Documents/Courses/2023_SEM_2/ML4QS/main/grud/experiments.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m missing_values_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(\u001b[39m~\u001b[39mtest_dataset\u001b[39m.\u001b[39mmask\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(\u001b[39mbool\u001b[39m)) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39mprod(test_dataset\u001b[39m.\u001b[39mmask\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lverm/Documents/Courses/2023_SEM_2/ML4QS/main/grud/experiments.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing values in training data: \u001b[39m\u001b[39m{\u001b[39;00mmissing_values_train\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_dataset' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Analyze the datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate label distribution\n",
    "train_label_distribution = np.bincount(train_dataset.y.flatten()) / len(train_dataset.y.flatten())\n",
    "val_label_distribution = np.bincount(val_dataset.y.flatten()) / len(val_dataset.y.flatten())\n",
    "test_label_distribution = np.bincount(test_dataset.y.flatten()) / len(test_dataset.y.flatten())\n",
    "\n",
    "# Plot label distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(np.arange(6), train_label_distribution, alpha=0.5, label='Train')\n",
    "plt.bar(np.arange(6), val_label_distribution, alpha=0.5, label='Validation')\n",
    "plt.bar(np.arange(6), test_label_distribution, alpha=0.5, label='Test')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Label Distribution')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate missing values\n",
    "missing_values_train = np.sum(~train_dataset.mask.numpy().astype(bool)) / np.prod(train_dataset.mask.shape)\n",
    "missing_values_val = np.sum(~val_dataset.mask.numpy().astype(bool)) / np.prod(val_dataset.mask.shape)\n",
    "missing_values_test = np.sum(~test_dataset.mask.numpy().astype(bool)) / np.prod(test_dataset.mask.shape)\n",
    "\n",
    "print(f\"Missing values in training data: {missing_values_train*100:.2f}%\")\n",
    "print(f\"Missing values in validation data: {missing_values_val*100:.2f}%\")\n",
    "print(f\"Missing values in testing data: {missing_values_test*100:.2f}%\")\n",
    "\n",
    "# Plot feature distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(train_dataset.x.shape[2]):\n",
    "    sns.histplot(train_dataset.x[:, :, i].numpy().flatten(), bins=50, kde=True)\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Feature Distribution in Training Data')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
